{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "# https://techwithtim.net/tutorials/ai-chatbot/part-4/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are doing with the JSON file is creating a bunch of messages that the user is likely to type in and mapping them to a group of appropriate responses. The tag on each dictionary in the file indicates the group that each message belongs too. With this data we will train a neural network to take a sentence of words and classify it as one of the tags in our file. Then we can simply take a response from those groups and display that to the user. The more tags, responses, and patterns you provide to the chatbot the better and more complex it will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each pattern we will turn it into a list of words using nltk.word_tokenizer, rather than having them as strings. We will then add each pattern into our patterns list and its associated tag into the tags list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(words)\\nprint(labels)\\nprint(patterns)\\nprint(tags)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "with open(\"intents.json\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "try:\n",
    "    with open(\"data.pickle\", \"rb\") as f:\n",
    "        words, labels, training, output = pickle.load(f)\n",
    "except:    \n",
    "    words = []\n",
    "    labels = []\n",
    "    patterns = [] # store words in each pattern\n",
    "    tags = [] # store tags of each pattern\n",
    "    ignore = ['?','!','.','$'] # ignore all of these symbols when processing the data as they are not needed\n",
    "    for intent in data[\"intents\"]:\n",
    "        for pattern in intent[\"patterns\"]:\n",
    "            # take each word and tokenize it\n",
    "            wrds = nltk.word_tokenize(pattern)\n",
    "            words.extend(wrds)\n",
    "            patterns.append(wrds)\n",
    "            tags.append(intent[\"tag\"])\n",
    "            # adding labels to label list\n",
    "        if intent[\"tag\"] not in labels:\n",
    "            labels.append(intent[\"tag\"])\n",
    "\n",
    "    words = [stemmer.stem(w.lower()) for w in words if w not in ignore] \n",
    "    # lemma gurantees root word is an actual word \n",
    "    # However this interfers with accuracy for whatever reason\n",
    "    words = sorted(list(set(words))) # set to avoid repeatition of words\n",
    "\n",
    "    labels = sorted(labels) # sorted alphabetically\n",
    "    \n",
    "\"\"\"\n",
    "print(words)\n",
    "print(labels)\n",
    "print(patterns)\n",
    "print(tags)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded in our data and created a stemmed vocabulary it's time to talk about a bag of words. As we know neural networks and machine learning algorithms require numerical input. So out list of strings wont cut it. We need some way to represent our sentences with numbers and this is where a bag of words comes in. What we are going to do is represent each sentence with a list the length of the amount of words in our models vocabulary. Each position in the list will represent a word from our vocabulary. If the position in the list is a 1 then that will mean that the word exists in our sentence, if it is a 0 then the word is nor present. We call this a bag of words because the order in which the words appear in the sentence is lost, we only know the presence of words in our models vocabulary.\n",
    "\n",
    "As well as formatting our input we need to format our output to make sense to the neural network. Similarly to a bag of words we will create output lists which are the length of the amount of labels/tags we have in our dataset. Each position in the list will represent one distinct label/tag, a 1 in any of those positions will show which label/tag is represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    training = []\n",
    "    output = []\n",
    "\n",
    "    out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "    for x, doc in enumerate(patterns):\n",
    "        bag = [] # initializing bag of words \n",
    "        \n",
    "         # lemmatize each word - create base word, in attempt to represent related words\n",
    "        wrds = [stemmer.stem(w.lower()) for w in doc]\n",
    "        \n",
    "    # create our bag of words array with 1, if word match found in current pattern\n",
    "        for w in words:\n",
    "            if w in wrds:\n",
    "                bag.append(1)\n",
    "            else:\n",
    "                bag.append(0)\n",
    "                \n",
    "        # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
    "        output_row = out_empty[:]\n",
    "        output_row[labels.index(tags[x])] = 1\n",
    "\n",
    "        training.append(bag)\n",
    "        output.append(output_row)\n",
    "\n",
    "\n",
    "    training = numpy.array(training)# convert training and output into numpy arrays\n",
    "    output = numpy.array(output)\n",
    "    #print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"data.pickle\", \"wb\") as f:\n",
    "        pickle.dump((words, labels, training, output), f)\n",
    "\n",
    "tf.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3999  | total loss: \u001b[1m\u001b[32m0.10635\u001b[0m\u001b[0m | time: 0.016s\n",
      "| Adam | epoch: 1000 | loss: 0.10635 - acc: 0.9891 -- iter: 24/31\n",
      "Training Step: 4000  | total loss: \u001b[1m\u001b[32m0.09712\u001b[0m\u001b[0m | time: 0.021s\n",
      "| Adam | epoch: 1000 | loss: 0.09712 - acc: 0.9902 -- iter: 31/31\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\123\\Documents\\Data Analytics\\projects\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "net = tflearn.fully_connected(net, 128)\n",
    "net = tflearn.fully_connected(net, 64)\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
    "net = tflearn.regression(net)\n",
    "# first 2 layers have 8 nuerons, output layer is equal to number of tags\n",
    "model = tflearn.DNN(net)\n",
    "\n",
    "if os.path.exists(\"model\" + \".meta\"):\n",
    "    model.load(\"model.tflearn\")\n",
    "else:\n",
    "    model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "    model.save(\"model.tflearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bag_of_words(s, words): # creates bag of words out of user's input to pass into model\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "\n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "            \n",
    "    return numpy.array(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start talking with my bot (type quit to stop)!\n",
      "You: hi\n",
      "Bot: Hello, thanks for visiting\n",
      "You: thanks\n",
      "Bot: Any time!\n",
      "You: bye\n",
      "Bot: Have a nice day\n",
      "You: name\n",
      "Fiona: My name is Fiona. Why? Because my creator thought it sounded nice.\n",
      "You: hobbies\n",
      "Fiona: My hobbies include playing video games, fitness (gym, biking etc.), basketball and watching sitcoms such as The Office, Brooklyn-99 and How I met your mother.\n",
      "You: data science\n",
      "Fiona: It started back in first year at a Loblaw networking session. That is when I was first introduced to data science. They described it as a puzzle and the more pieces you fit the clearer the picture got, and you would gain insight from different perspectives. Recently however, I have been wanting to better my software development skills as I want to be more well-rounded than only having 1 specialization/concentration.\n",
      "You: peers\n",
      "Fiona: He is a take charge, self managed leader. He is very hard working and will put the time in. Always setting strict deadlines to push out work in an efficient manner.\n",
      "You: 5 years\n",
      "Fiona: Professionally it would be learning new things/skills. Gain confidence in my abilities and skills. Pay off my loan and get my masters. On the personal side it would be getting into shape and potentially finding someone to settle down and start a family with.\n",
      "You: purpose\n",
      "Fiona: To showcase my creators skills and why you should hire him. Please do so I can be put out of my misery!\n",
      "You: strength\n",
      "Fiona: My biggest strength is that I am adaptable. Something I could not claim until recently. As this past summer was a research assistant for my professor. However, as I person that learns best in person/face to face and my professor travelling due to other commitments. I had to adapt and learn on my own fairly quickly as I had to send weekly reports on what I had researched. My weakness is that I tend to overthink things. Sometimes when Iâ€™m seeing something new or the task ahead is huge, I overthink how complicated the issue really is. I always go back to what one of my professors said and thatâ€™s divide and conquer.\n",
      "You: sonsonosc\n",
      "Fiona: Good to see you again\n",
      "You: \n",
      "Fiona: Good to see you again\n",
      "You: yourself\n",
      "Fiona: Growing up I did not know what I wanted to be. At 8 years old and after asking my mom I wanted to be an electrical engineer. As the years went by anytime my family got some sort of new technology,I was the one setting it up and trouble shooting any issues.That started my interest in technology primarily computers.So, at that point I wanted to be a computer engineer. By taking certain electives in high school I liked the software side more and by doing my research about the field I felt that studying computer science would be the best path. As I would have more electives to choose from than if I studied software engineering.\n",
      "You: quit\n"
     ]
    }
   ],
   "source": [
    "def chatbot():\n",
    "    print(\"Start talking with my bot (type quit to stop)!\")\n",
    "    name = \"Bot: \"\n",
    "    while True:\n",
    "        inp = input(\"You: \")\n",
    "        if inp.lower() == \"quit\":\n",
    "            break\n",
    "\n",
    "        results = model.predict([bag_of_words(inp, words)])\n",
    "        results_index = numpy.argmax(results)\n",
    "        tag = labels[results_index]\n",
    "\n",
    "        for t in data[\"intents\"]:\n",
    "            if t['tag'] == tag:\n",
    "                responses = t['responses']\n",
    "            if tag == \"name\":\n",
    "                name = \"Fiona: \"\n",
    "        print(name,end=\"\")\n",
    "        print(random.choice(responses))\n",
    "\n",
    "chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
